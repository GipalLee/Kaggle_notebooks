{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepfake 1.1.3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comb0601/Kaggle-Notebooks/blob/main/deepfake_1_1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQxoH1dTqveo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e574a8-ea0c-4314-b76b-0abf62659910"
      },
      "source": [
        "!pip install mtcnn\n",
        "!pip install facenet-pytorch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import measure\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n",
            "Collecting facenet-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/e9/9dee8c3924478283d0f29c7b36b9d000a4e6928c00d8108e66df494218e6/facenet_pytorch-2.5.1-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from facenet-pytorch) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from facenet-pytorch) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from facenet-pytorch) (0.8.1+cu101)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->facenet-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision->facenet-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision->facenet-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision->facenet-pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision->facenet-pytorch) (0.8)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD1xSyyMjEp7",
        "outputId": "113018a9-7eea-4c90-f6f2-65e8fc90badc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2uxfGk6qvaM"
      },
      "source": [
        "#TEST_PATH = '/content/'\n",
        "TRAIN_PATH = '/gdrive/My Drive/'\n",
        "\n",
        "#metadata = '../input/deepfake-detection-challenge/train_sample_videos/metadata.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFuafGSNqvW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f120a3-26f6-4577-b8d6-b9700e02c8e9"
      },
      "source": [
        "# load the filenames for train videos\n",
        "train_fns = sorted(glob.glob(TRAIN_PATH + '*.mp4'))\n",
        "\n",
        "# load the filenames for test videos\n",
        "#test_fns = sorted(glob.glob(TEST_PATH + '*.mp4'))\n",
        "\n",
        "print('There are {} samples in the train set.'.format(len(train_fns)))\n",
        "#print('There are {} samples in the test set.'.format(len(test_fns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 samples in the train set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJIwh_TB_bfp",
        "outputId": "953df23e-5820-434c-a0bd-dfc7d6872b70"
      },
      "source": [
        "train_fns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/fake.mp4', '/gdrive/My Drive/real.mp4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blTehhxFqvUr"
      },
      "source": [
        "def get_frames(filename):\n",
        "    '''\n",
        "    Get all frames from the video\n",
        "    INPUT:\n",
        "        filename - video filename\n",
        "    OUTPUT:\n",
        "        frames - the array of video frames\n",
        "    '''\n",
        "    frames = []\n",
        "    \n",
        "    cap = cv2.VideoCapture(filename)\n",
        "    \n",
        "    #while(cap.isOpened()):\n",
        "    while len(frames) < 301 :\n",
        "        ret, frame = cap.read()\n",
        "                \n",
        "        if not ret:\n",
        "            break;\n",
        "            \n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(image)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return frames\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpiUq5gQ0kkH"
      },
      "source": [
        "# 얼굴 감지\n",
        "# Create face detector\n",
        "for video in train_fns[:1] :\n",
        "  frames = get_frames(video)\n",
        "  mtcnn = MTCNN(margin=20, keep_all=False, post_process=False,device='cuda:0')  #keep_all : multiple faces in a single image,device='cuda:0'\n",
        "\n",
        "  \n",
        "  save_paths = [str(video)+f'image_{i}.jpg' for i in range(len(frames[:100]))]\n",
        "  mtcnn(frames[:100], save_path=save_paths)\n",
        "\n",
        "  save_paths = [str(video)+f'image_{i}.jpg' for i in range(len(frames[100:200]))]\n",
        "  mtcnn(frames[100:200], save_path=save_paths)\n",
        "\n",
        "  save_paths = [str(video)+f'image_{i}.jpg' for i in range(len(frames[200:300]))]\n",
        "  mtcnn(frames[200:300], save_path=save_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By-780qeeWt-"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OcTZvS0yCxU7"
      },
      "source": [
        "def calcRGBCenter(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #RGB 형태로 변환\n",
        "    image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
        "    k = 1\n",
        "    clt = KMeans(n_clusters = k)\n",
        "    clt.fit(image)\n",
        "\n",
        "    return clt.cluster_centers_[0]\n",
        "\n",
        "\n",
        "def calcHSVCenter(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) #HSV 형태로 변환\n",
        "    image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
        "    k = 1\n",
        "    clt = KMeans(n_clusters = k)\n",
        "    clt.fit(image)\n",
        "\n",
        "    return clt.cluster_centers_[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfmgjPjGLZV"
      },
      "source": [
        "def get_image_difference(image_1, image_2):  # 히스토그램 차이\n",
        "    first_image_hist = cv2.calcHist([image_1], [0], None, [256], [0, 256])\n",
        "    second_image_hist = cv2.calcHist([image_2], [0], None, [256], [0, 256])\n",
        "\n",
        "    img_hist_diff = cv2.compareHist(first_image_hist, second_image_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
        "    #img_template_probability_match = cv2.matchTemplate(first_image_hist, second_image_hist, cv2.TM_CCOEFF_NORMED)[0][0]\n",
        "    #img_template_diff = 1 - img_template_probability_match\n",
        "\n",
        "    # taking only 10% of histogram diff, since it's less accurate than template method\n",
        "    #commutative_image_diff = (img_hist_diff / 10) + img_template_diff\n",
        "    return img_hist_diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cgRnYRuTAoOt",
        "outputId": "b65c0d6f-1f34-438d-c595-1b12b4371dcc"
      },
      "source": [
        "train_fns[0] + 'image_1.jpg'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/My Drive/fake.mp4image_1.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "15rbD9mX_GH2",
        "outputId": "3cab69de-474c-4c1e-8225-b6b4ab7f9c87"
      },
      "source": [
        "videoname"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fake.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Ajhfu2xJIX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "1ad7a9e9-04bc-4359-d1b8-ca691346976d"
      },
      "source": [
        "    #for videoname in train_fns : # 하나의 동영상 이름 가져오기\n",
        "    videoname = train_fns[0]\n",
        "    frames = []\n",
        "    is_fake = True\n",
        "    #if videoname.startswith(\"fake\"):\n",
        "    #     is_fake = True\n",
        "\n",
        "    for i in range(100) :      # 가져온 동영상 얼굴 사진들로 프레임 생성\n",
        "      frame = cv2.imread(videoname+'image_'+str(i)+'.jpg')\n",
        "      frames.append(frame)\n",
        "\n",
        "    resfile = open(videoname+\"_result.csv\", \"a\")\n",
        "    resfile.write(\"mse,psnr,ssim,hist_diff,r_diff,g_diff,b_diff,h_diff,s_diff,v_diff,deepfake\\n\")\n",
        "\n",
        "    for i in range(len(frames)-1) :\n",
        "        img1 = frames[i] # 첫 프레임\n",
        "        img2 = frames[i+1] #다음 프레임\n",
        "\n",
        "        mse = np.sum((img1.astype(\"float\") - img2.astype(\"float\")) ** 2) #MSE\n",
        "        mse /= float(img1.shape[0] * img1.shape[1])\n",
        "\n",
        "        psnr = cv2.PSNR(img1, img2) # 프레임 간 PSNR값\n",
        "\n",
        "        ssim = measure.compare_ssim(img1, img2,multichannel=True) # SSIM 멀티채널 오픈필요\n",
        "\n",
        "        histogram_diff = get_image_difference(img1,img2) # 프레임간 히스토그램 차이\n",
        "\n",
        "        rgb_diff = abs(calcRGBCenter(img2) - calcRGBCenter(img1))\n",
        "\n",
        "        hsv_diff = abs(calcHSVCenter(img2) - calcHSVCenter(img1))\n",
        "\n",
        "        resfile.write(\n",
        "            str(mse) + ',' + str(psnr) + ',' + str(ssim) + ',' + str(histogram_diff) + ',' +\n",
        "            str(rgb_diff[0]) + ',' + str(rgb_diff[1]) + ',' + str(rgb_diff[2]) + ',' +\n",
        "            str(hsv_diff[0]) + ',' + str(hsv_diff[1]) + ',' + str(hsv_diff[2]) + ',' + str(is_fake) + '\\n'\n",
        "        )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e7548a2ad5a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#다음 프레임\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV9kntiPgi3A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}